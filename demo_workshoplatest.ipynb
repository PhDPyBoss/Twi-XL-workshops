{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; vertical-align: middle; margin: 1em;\" src=\"images/surf.png\" >\n",
    "<img style=\"float: right; height: 5em; vertical-align: middle; margin: 1em;\" src=\"images/twitter.png\">\n",
    "\n",
    "<hr style=\"clear: both;\" />\n",
    "\n",
    "# Twi-XL TwiNL collection demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twi-XL contains a Twitter archive called TwiNL, which has been maintained by [SURF](https://www.surf.nl/en) since 2011. The Twi-XL API exposes functionality for searching tweets and counting word frequencies. This notebook provides a brief overview of this functionality through the [Twi-XL Python library](https://gitlab.com/twi-xl-surf-nl/twi-xl-python).\n",
    "\n",
    "Check the [Architecture Overview](https://twi-xl-python.readthedocs.io/en/latest/architecture_overview.html) of the Twi-XL API.\n",
    "\n",
    "Check the [ReadtheDocs](https://twi-xl-python.readthedocs.io/en/latest/api.html#main-interface) of the Twi-XL Python library.\n",
    "\n",
    "## Prerequisites\n",
    "### Installing Python libraries \n",
    "To install the Twi-XL Python library and the dependencies we will need in this notebook, run the following cell:\n",
    "\n",
    "(**Warning: this might take a while**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet seaborn spacy tqdm tweepy snscrape wordcloud tldextract\n",
    "!pip3 install --quiet git+https://gitlab.com/twi-xl-surf-nl/twi-xl-python.git@b8b301ca707ac46ce8eb9f7cb31035528c25385a\n",
    "\n",
    "!python3 -m spacy download nl_core_news_sm  # this will download the Dutch language pipeline for Spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python libraries\n",
    "The `twinl` package from the Python Twi-XL library provides all functionality for interfacing with the TwiNL archive. Run the following cell to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twixl.collections import twinl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the Twi-XL library, we import all packages that we might need later for executing our code. Python packages can provide different in-built functionalities, such as plotting graphs or word clouds, handling date formats, creating and reading csv files, or handling particular types of data formats like JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import twixl.collections.twinl.plotting\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from urllib.parse import urlparse\n",
    "import tldextract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the API\n",
    "The Twi-XL Python library needs some information to communicate with the Twi-XL API. We will set two environment variables, one containing the endpoint (URL) of the Twi-XL API, and another containing an API key that is used to authenticate with the API.\n",
    "\n",
    "Please add your API key directly in the Python code, without any strings or additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TWIXL_API_ENDPOINT=https://api.twi-xl.sda-projects.nl\n",
    "%env TWIXL_API_KEY="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive metrics\n",
    "We're all set! First, let's have a look at the number of tweets collected since the beginning. We'll use the `tweet_metrics()` function to retrieve the number of tweets for each day in the archive and convert them to a Pandas series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_metrics = twinl.tweet_metrics()\n",
    "tweet_metrics.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the tweet metrics using the `plot_tweet_metrics()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lines below configure the environment for plotting, we only need to do this once\n",
    "\n",
    "sns.set(rc={'figure.figsize': (14, 8)})\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "twinl.plotting.plot_tweet_metrics(tweet_metrics);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: some tweets in the archive are marked as being created before the inception of the TwiNL archive, some of them even from 1994! The archive contains raw data from Twitter and can contain these kinds of inconsistencies as a result.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query example 1: OR query\n",
    "In order to search for tweets we will first need to construct a query. We can do this with the `Query` class in the `twinl` module. As an example, consider the following query designed to find tweets containing the words 'elfstedentocht' **or** 'schaatsen':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_example_1 = (\n",
    "    twinl.Query()\n",
    "        .or_(keywords=[\"elfstedentocht\", \"schaatsen\"])\n",
    ")\n",
    "\n",
    "query_example_1.print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `or_()` method used above specifies that a query matches if a tweet contains any of the words in the list provided by the `keyword` parameter. The `print()` method prints the contents of the query that will be sent to the Twi-XL API endpoint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query example 2: AND query\n",
    "We can also write queries where all keywords must be present in a tweet. Consider the following example, where the words 'elfsteden' and 'tocht' must both appear in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_example_2 = (\n",
    "    twinl.Query()\n",
    "        .and_(keywords=[\"elfsteden\", \"tocht\"])\n",
    ")\n",
    "\n",
    "query_example_2.print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query example 3: AND + OR query\n",
    "Query criteria such as AND and OR can be combined by chaining operators such as `and_()` and `or_()` on the `Query` object. When specifying a query with multiple criteria the query will match if **any** of the criteria matches. For example, consider the following query where we will match tweets if any of the following criteria apply:\n",
    "\n",
    "1. The tweet contains the word 'elfstedentocht' OR 'schaatsen';\n",
    "2. The tweet contains the word 'elfsteden' AND 'tocht'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_example_3 = (\n",
    "    twinl.Query()\n",
    "        .or_(keywords=[\"elfstedentocht\", \"schaatsen\"])\n",
    "        .and_(keywords=[\"elfsteden\", \"tocht\"])\n",
    ")\n",
    "\n",
    "query_example_3.print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query example 4: regular expressions\n",
    "With regular expressions we can create more flexible queries. Consider the following example, where we search for any tweet containing words starting with 'elf' or 'schaats':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_example_4 = (\n",
    "    twinl.Query()\n",
    "        .or_(keywords=[\"\\belf\\w+\", \"\\bschaats\\w+\"], regex=True)\n",
    ")\n",
    "\n",
    "query_example_4.print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query example 5: combining AND, OR and regular expressions\n",
    "For another example, consider this query that finds tweets matching any of the following criteria:\n",
    "\n",
    "1. The tweet contains the words 'elfstedentocht' or 'schaatsen';\n",
    "1. The tweet contains words starting with 'elf' or 'schaatsen';\n",
    "1. The tweet contains the word 'elfsteden' and 'tocht'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_example_5 = (\n",
    "    twinl.Query()\n",
    "        .or_(keywords=[\"elfstedentocht\", \"schaatsen\"])\n",
    "        .or_(keywords=[\"\\belf\\w+\", \"\\bschaats\\w+\"], regex=True)\n",
    "        .and_(keywords=[\"elfsteden\", \"tocht\"])\n",
    ")\n",
    "\n",
    "query_example_5.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_custom = (\n",
    "    twinl.Query()\n",
    "        .or_(keywords=[\"#verkiezingen\"])\n",
    ")\n",
    "\n",
    "query_custom.print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with the Twi-XL API\n",
    "Apart from a search query (we will use our custom query) we will also need to provide the API with a time range, consisting of a **start date** and time and **end date** and time. \n",
    "\n",
    "Optionally, we can specify the **maximum** number of tweets we want returned in our query. We can remove the parameter if we want all results.\n",
    "\n",
    "Tweets are returned in **chronological** order. If a maximum number *x* is set, we retrieve the earliest *x* tweets. \n",
    "\n",
    "The following cell runs our last query on the TwiNL archive between January 1 and January 23, 2017 using the `search()` function. Because searching can take a while, we provide a so-called callback function to the `search()` function. This function will be called every few seconds with the current status of the query. The `twinl.print_callback` in the cell below is a default callback that simply prints the current query status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = twinl.search(\n",
    "    query=query_custom,\n",
    "    start_time=datetime(2017, 1, 1, 0, 0),\n",
    "    end_time=datetime(2017, 1, 22, 23, 59, 59),\n",
    "    max_results=100,\n",
    "    callback=twinl.print_callback\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas integration\n",
    "Due to Twitter user policy restrictions the Twi-XL API returns only metadata of the tweets such as their IDs and timestamps. The search results can be converted to a Pandas data frame using the `to_pandas()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = search_results.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas data frame can be written to a csv file. To change the filename you can simply edit it in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"demo.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency plot\n",
    "The `twinl.plotting` module contains some example functions to plot search results. In order to see how tweets are distributed over time we can plot the frequencies of the tweets using the `plot_tweet_frequencies()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twinl.plotting.plot_tweet_frequencies(search_results, title=\"Number of '#verkiezingen' tweets per day\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencies from the TwiNL database\n",
    "#### Daily \n",
    "Apart from searching for tweets we can also lookup daily word frequencies using the `twinl.word_frequency()` function. For a word frequency search we have to specify a date and, optionally, the minimum word length (default 1), the number of words returned (default is all) and the minimum occurrence rate of words (default is 1).\n",
    "\n",
    "In the following cell, we retrieve word frequencies for January 4 2017 with a minimum word length of 5, a minimum occurrence rate (`frequency_limit`) of 100, and 50,000 maximum words returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = twinl.word_frequency(\n",
    "    date=datetime(2017, 1, 4),\n",
    "    min_length_words=5,\n",
    "    max_results=50000,\n",
    "    frequency_limit=100,\n",
    "    callback=twinl.print_callback\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas data frame can be written to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = word_frequencies.to_pandas()\n",
    "df.to_csv(\"demo_word_freq_example.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twi-XL word cloud\n",
    "To visualize the daily word frequencies we can create a word cloud. We will first filter the results with a list of known Dutch stopwords provided by the [Spacy](https://spacy.io/usage/spacy-101) natural language processing library, and plot a word cloud with the `plotting.plot_word_cloud()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = spacy.load(\"nl_core_news_sm\")\n",
    "stopwords = nl.Defaults.stop_words\n",
    "\n",
    "twinl.plotting.plot_word_cloud(word_frequencies, stopwords=stopwords, max_words=100, min_word_length=4);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Twi-XL circular plot \n",
    "#### Hourly \n",
    "To visualize the most-tweeted words per hour in a day we can use the `plotting.plot_circular_bars()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twinl.plotting.plot_circular_bars(word_frequencies, stopwords=stopwords, group_size=2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet analysis\n",
    "### Retrieval \n",
    "The Twi-XL API provides so far only tweet ids and timestamp, or aggregate data. \n",
    "\n",
    "To access the full tweets with more metadata we make use of a scraping library called [snscrape](https://github.com/JustAnotherArchivist/snscrape).\n",
    "\n",
    "We first store all tweet Objects in a list, which we will process further below. A tweet Object has different properties that are defined by the scraping tool (and not by the Twitter API as usual although they overlap), such as hashtags, links, user, replyCount, retweetCount, etc. (see [here](https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py) for an overview starting from line 100). Each tweet is stored in a \"dictionary\", a data structure in Python to store objects by unique keys and values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "first_x_tweets = 10\n",
    "tweet_ids = [tweet_metadata.tweet_id for tweet_metadata in search_results.tweets[:first_x_tweets]]\n",
    "for tweet_id in tweet_ids:\n",
    "    for i,tweet in enumerate(sntwitter.TwitterTweetScraper(tweetId=tweet_id,mode=sntwitter.TwitterTweetScraperMode.SINGLE).get_items()):\n",
    "        print(tweet)\n",
    "        if type(tweet) is sntwitter.Tweet:\n",
    "            tweet_dict = tweet.__dict__\n",
    "            tweets.append(tweet_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store tweets in CSV \n",
    "We can store the scraped tweets with the all the delivered metadata in a csv file as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweets.csv', 'w') as tweets_csv:\n",
    "    writer = csv.writer(tweets_csv)\n",
    "    writer.writerow(tweets[0].keys())\n",
    "    for tweet_dictionary in tweets:\n",
    "        writer.writerow(tweet_dictionary.values())\n",
    "    tweets_csv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags\n",
    "### Hashtags per tweet\n",
    "First, we can simply print the hashtags that occur within each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    tweet_hashtags_list = tweet['hashtags']\n",
    "    if tweet_hashtags_list is not None:\n",
    "        tweet_hashtags_string = ', '.join(tweet_hashtags_list)\n",
    "        print('Hashtags for tweet ' + tweet['url'] + ': ' + tweet_hashtags_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag frequencies\n",
    "\n",
    "Then, we look into which hashtags are occuring overall and how often. For this we use again a dictionary to store the hashtags keys and the value of how often they occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = {}\n",
    "for tweet in tweets:\n",
    "    tweet_hashtags_list = tweet['hashtags']\n",
    "    for hashtag in tweet_hashtags_list:\n",
    "        if hashtag in hashtags:\n",
    "            hashtags[hashtag] = hashtags[hashtag] + 1\n",
    "        else:\n",
    "            hashtags[hashtag] = 1\n",
    "\n",
    "hashtags_sorted = sorted(hashtags.items(), key=lambda x:x[1], reverse=True)\n",
    "for hashtag in hashtags_sorted:\n",
    "    print(hashtag[0] + ': ' + str(hashtag[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store these values in a csv with executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hashtags_frequencies.csv','w') as file:\n",
    "    csv_out=csv.writer(file)\n",
    "    csv_out.writerow(['hashtag','count'])\n",
    "    for row in hashtags_sorted:\n",
    "        csv_out.writerow(row)\n",
    "    file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag cloud\n",
    "\n",
    "We can also plot a hashtag cloud that visualizes how often which hashtags appear for our search query. For this we use the Python [wordcloud](https://github.com/amueller/word_cloud) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all hashtags into one text\n",
    "tweet_hashtags_list = []\n",
    "for tweet in tweets:\n",
    "    tweet_hashtags_list.append(' '.join(tweet['hashtags']))\n",
    "text = ' '.join(tweet_hashtags_list)\n",
    "\n",
    "# a default list of stopwords is used; one can add individual ones with stopwords.add('word')\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# create world cloud object\n",
    "wc = WordCloud(max_words=1000, stopwords=stopwords, margin=10,\n",
    "               random_state=1).generate(text)\n",
    "\n",
    "# store default colored image\n",
    "default_colors = wc.to_array()\n",
    "wc.to_file(\"hashtag_cloud.png\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Hashtag cloud\")\n",
    "plt.imshow(default_colors, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs\n",
    "\n",
    "In the following, we are going to print the URLs shared for individual tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    links = tweet['links']\n",
    "    if links is not None:\n",
    "        urls = [l.url for l in links]\n",
    "        for url in urls:\n",
    "            print('Urls for tweet ' + tweet['url'] + ': ' + url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {}\n",
    "for tweet in tweets:\n",
    "    links = tweet['links']\n",
    "    if links is not None:\n",
    "        tweet_urls_list = [l.url for l in tweet['links']]\n",
    "        for url in tweet_urls_list:\n",
    "            if url in urls:\n",
    "                urls[url] = urls[url] + 1\n",
    "            else:\n",
    "                urls[url] = 1\n",
    "\n",
    "urls_sorted = sorted(urls.items(), key=lambda x:x[1], reverse=True)\n",
    "for url in urls_sorted:\n",
    "    print(url[0] + ': ' + str(url[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store these values in a csv with executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url_frequencies.csv','w') as file:\n",
    "    csv_out=csv.writer(file)\n",
    "    csv_out.writerow(['url','count'])\n",
    "    for row in urls_sorted:\n",
    "        csv_out.writerow(row)\n",
    "    file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain frequencies\n",
    "\n",
    "Each URL has a particular domain which can be studied to detect overall trends in dominant sites. For this we make use of the [tldextract](https://pypi.org/project/tldextract/) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = [tldextract.extract(url).domain for url in urls.keys()]\n",
    "domains = {}\n",
    "for domain in domains_list:\n",
    "    if domain in domains:\n",
    "        domains[domain] = domains[domain] + 1\n",
    "    else:\n",
    "        domains[domain] = 1\n",
    "    \n",
    "domains_sorted = sorted(domains.items(), key=lambda x:x[1], reverse=True)\n",
    "for domain in domains_sorted:\n",
    "    print(domain[0] + ': ' + str(domain[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store these values in a csv with executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('domain_frequencies.csv','w') as file:\n",
    "    csv_out=csv.writer(file)\n",
    "    csv_out.writerow(['domain','count'])\n",
    "    for row in domains_sorted:\n",
    "        csv_out.writerow(row)\n",
    "    file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain clouds\n",
    "\n",
    "Plotting clouds for individual URLs is not ideal to keep oversight. However, for domains, the cloud visualization is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(list(domains.keys()))\n",
    "\n",
    "# create world cloud object\n",
    "wc = WordCloud(max_words=1000, margin=10,\n",
    "               random_state=1).generate(text)\n",
    "\n",
    "# store default colored image\n",
    "default_colors = wc.to_array()\n",
    "wc.to_file(\"domain_cloud.png\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Domain cloud\")\n",
    "plt.imshow(default_colors, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country code frequencies\n",
    "Sometimes it is useful to see which country codes occur most frequently amongst shared links. For this, we retrieve all occurring country codes from URLs and sort them by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_list = [tldextract.extract(url).suffix for url in urls.keys()]\n",
    "country_codes = {}\n",
    "for country_code in country_code_list:\n",
    "    if country_code in country_codes:\n",
    "        country_codes[country_code] = country_codes[country_code] + 1\n",
    "    else:\n",
    "        country_codes[country_code] = 1\n",
    "    \n",
    "country_codes_sorted = sorted(country_codes.items(), key=lambda x:x[1], reverse=True)\n",
    "for country_code in country_codes_sorted:\n",
    "    print(country_code[0] + ': ' + str(country_code[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can store the results in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('country_code_frequencies.csv','w') as file:\n",
    "    csv_out=csv.writer(file)\n",
    "    csv_out.writerow(['country code','count'])\n",
    "    for row in country_codes_sorted:\n",
    "        csv_out.writerow(row)\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country code clouds\n",
    "Or, we can display occurring country codes in a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(list(country_codes.keys()))\n",
    "\n",
    "# create world cloud object\n",
    "wc = WordCloud(max_words=1000, margin=10,\n",
    "               random_state=1).generate(text)\n",
    "\n",
    "# store default colored image\n",
    "default_colors = wc.to_array()\n",
    "wc.to_file(\"country_code_cloud.png\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Country code cloud\")\n",
    "plt.imshow(default_colors, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs and users\n",
    "\n",
    "In the following, we retrieve all URLs per user for the given search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_per_user = {}\n",
    "for tweet in tweets:\n",
    "    if tweet['user'] is not None:\n",
    "        user = tweet['user'].username\n",
    "        if tweet['links'] is not None:\n",
    "            links = [t.url for t in tweet['links']]\n",
    "            print(links)\n",
    "            if user in links_per_user:\n",
    "                links_per_user[user] = links_per_user[user].append(links)\n",
    "            else:\n",
    "                links_per_user[user] = links\n",
    "\n",
    "print(links_per_user)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store this output as a csv with the columns **user** and **urls**, and we separate multiple URLs by a semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls_users.csv','w') as file:\n",
    "    w = csv.writer(file)\n",
    "    w.writerow(['user', 'url'])\n",
    "    for user in links_per_user:\n",
    "        w.writerow([user, ' ; '.join(links_per_user[user])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domains and users\n",
    "We can repeat the same inquiry on the domain level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_per_user = {}\n",
    "for tweet in tweets:\n",
    "    if tweet['user'] is not None:\n",
    "        user = tweet['user'].username\n",
    "        if tweet['links'] is not None:\n",
    "            domains = [tldextract.extract(t.url).domain for t in tweet['links']]\n",
    "            print(domains)\n",
    "            if user in domains_per_user:\n",
    "                domains_per_user[user] = domains_per_user[user].append(domains)\n",
    "            else:\n",
    "                domains_per_user[user] = domains\n",
    "\n",
    "print(domains_per_user)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the csv output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('domains_users.csv','w') as file:\n",
    "    w = csv.writer(file)\n",
    "    w.writerow(['user', 'domain'])\n",
    "    for user in domains_per_user:\n",
    "        w.writerow([user, ' ; '.join(domains_per_user[user])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word clouds\n",
    "In the following, we will create word cloud showing the most dominant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = spacy.load(\"nl_core_news_sm\")\n",
    "stopwords = nl.Defaults.stop_words\n",
    "stopwords.add('https')\n",
    "texts = [t['rawContent'] for t in tweets]\n",
    "wordcloud = WordCloud(stopwords=stopwords, min_word_length=4).generate(' '.join(texts))\n",
    "plt.figure(figsize=(14, 8))\n",
    "# No axis details\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twixl-aixwVW7P-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfed756800931c910bb646c5f7d2ddc530c7da4704ef6d3c0686a260588f9dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
